\chapter{Teorie}

\section{Lattice theory}
\begin{definition}
    Let $B$ be a matrix with rows linearly independant rows $b_i \in \Rbb^d$, then the discrete subgroup $\Lambda(B) = \{\sum v_i b_i | v_i \in \Zbb\}$ is called a \textit{lattice}.   
\end{definition}

Let $\pi_i : \Rbb^d \to \operatorname{span}(b_0, \ldots, b_{i-1})^\perp$ be the orthogonal projection into the complement. In particular, $\pi_0 \equiv id$. Then the \textit{Gram-Schmidt orthogonalization} (GSO) of B is $B^* = (b_0, \ldots, b_{i-1})$, where $b^*_i = \pi_i(b_i) = b_i - \sum_{j=0}^{i-1} \mu_{i,j} \cdot b_j^*$ and $\mu_{i,j} = \langle \boldsymbol{b}_i, \boldsymbol{b}_j^* \rangle / \langle \boldsymbol{b}_j^*, \boldsymbol{b}_j^* \rangle$. 

Let $|| \cdot ||$ be the euclidean norm. Denote by $\lambda_i(\Lambda)$ the radius of theh smallest ball centered at the origin containing at least $i$ linearly independant lattice vectors. In particular, $\lambda_1(\Lambda)$ is the norm of the shortest vector of $\Lambda$.

Next we define the Gaussian heuristic to aproximate the shortest vector of a lattice.

\begin{definition}
    Let $\Lambda(B)$ be a lattice. Denote by $\operatorname*{vol}(\Lambda) = \operatorname*{det}(B)$ the determinant of the basis and $\mathbb{B}_d(R)$ the $d$-dimensional euclidean ball. Then 
    \begin{equation*}
        \mathrm{gh}(\Lambda) = \left(\frac{\mathrm{Vol}(\Lambda)}{\mathrm{Vol}(\mathfrak{B}_d(1))}\right)^{1/d} = \frac{\Gamma\left(1+\frac{d}{2}\right)^{1/d}}{\sqrt{\pi}} \cdot \mathrm{Vol}(\Lambda)^{1/d} \approx \sqrt{\frac{d}{2\pi e}} \cdot \mathrm{Vol}(\Lambda)^{1/d}
    \end{equation*}
    is called the \textit{Gaussian heuristic}.
\end{definition}

The main problem in lattice thoery is to find the shortest vector of a lattice.

\begin{definition}[Shortest Vector Problem (SVP)]
    Let $\Lambda(B)$ be a lattice. Find the shortest nonzero vector in $\Lambda(B)$.
\end{definition}

We will be interested in finding closest vector to the lattice which is guaranteed to not be too far away from the lattice.

\begin{definition}[$\alpha$-Bounded Distance Decoding (BDD$_\alpha$)]
    Given a lattice $\Lambda(B)$, a vector $t$ and a parameter $ \alpha > 0$ such that the euclidean distance between $t$ and the lattice $\operatorname*{dist}(t,B) < \alpha \cdot \lambda_1(\Lambda(B))$, find the lattice vector $v \in \Lambda(B)$ closest to $t$.
\end{definition}

To guarantee a unique solution, it is required that $\alpha < 1/2$. There is a generalization of the problem for $1/2 < \alpha < 1$, where we want to find a unique solution with high probability. Asymptotically, for any polynomially-bounded $\gamma \geq 1$ there is a reduction from BDD$_{1/\sqrt{2}\gamma}$ to $uSVP_\gamma$ from the following definition.

\begin{definition}[$\gamma$-Unique Shortest Vector Problem(uSVP$_\gamma$)]
    Let $\Lambda$ be a lattice such that $\lambda_2(\Lambda) > \gamma \cdot \lambda_1(\Lambda)$, find a nonzero vector $v \in \Lambda$ of length $\lambda_1(\Lambda)$.
\end{definition}

The mentioned reduction is due to Kannan's embedding, that constructs

\begin{equation*}
    L = \begin{pmatrix} 
        B & 0 \\ 
        t & \tau 
        \end{pmatrix}
\end{equation*}

where $\tau$ is some embedding factor. If $v$ is the closest vector to $t$, then the lattice $\Lambda(L)$ contains $(t - v, \tau)$, which is small.

We will need some lattice algorithms.

\begin{definition}[Enumeration]
    Consider the following problem: Given a matrix $B$ and a bound $R$, find all lattice vectors $v = \sum_{i=0}^{d-1} u_i \cdot b_i |_{u_i \in \Zbb}$ with some $u_i \not = 0$ and $\|v\|^2 < R^2$. Then by lattice vector enumeration we can pick the smallest one and solve the SVP.

    We can rewrite the vector $v$ with the Gram-Schmidt basis:
    \begin{align*}
        v = \sum_{i=0}^{d-1} u_i \cdot b_i &= \sum_{i=0}^{d-1} u_i \cdot \left( b_i^* + \sum_{j=0}^{i-1} \mu_{i,j} \cdot b_j^* \right) = \sum_{j=0}^{d-1} \left( u_j + \sum_{i=j+1}^{d-1} u_i \cdot \mu_{i,j} \right) \cdot b_j^*.
    \end{align*}
    And thanks to orthogonality, the norms of the projections $\pi_k(v)$ become
    \begin{equation*}
        \| \pi_k(v) \|^2 = \left\| \sum_{j=k}^{d-1} \left( u_j + \sum_{i=j+1}^{d-1} u_i \, \mu_{i,j} \right) b_j^* \right\|^2 = \sum_{j=k}^{d-1} \left( u_j + \sum_{i=j+1}^{d-1} u_i \, \mu_{i,j} \right)^2 \cdot \| b_j^* \|^2.
    \end{equation*}
    So the norms play nicely with the parameter $k$. Begin with finding $\pi_d(v)$ such that $\| \pi_d(v) \|^2 < R^2$
    and iterate the inequality over $d$. This defines a depth-first tree search. We find a candidate for $u_{d-1}$ and continue to $u_{d-2}$ level. Whenever we encounter no candidates, we abandon the branch and backtrack. When we reach the leaves $u_0$, we compare the candidates to the previously smallest found vector and backtrack.
\end{definition}

\begin{definition}[Sieving]
    The lattice sieve algorithm takes a set of lattice vectors $L \subset \Lambda$ and searches for integer combinations that are short. By recursively doing this process we can solve the SVP.
\end{definition}

\begin{definition}[LLL]
    
\end{definition}
\begin{definition}[BKZ]
    
\end{definition}


\section{The Hidden Number Problem}

\begin{definition}
Let $q$ be prime, $x$ a secret integer and $T_b = (q-1)/2^b$. An oracle generates random, uniformly distributed $c_j \in [1,\ldots, q-1],$ $k_j \in [-\nint{T_{b+1}}, \ldots, \floor{T_{b+1}}]$ and computes 
\begin{equation}\label{eqHNP}
    h_j = (k_j - c_j \cdot x) \mod q.
\end{equation}
The adversery is given the pairs $(h_j, c_j)$, $0<j<L$ and the goal is to recover $x$. We call this an instance of the \textit{hidden number problem} with a leak of $b$-bits.
\end{definition}

Some leaks in the (EC)DCA and Diffie-Hellman can be mapped to the HNP, which is traditionally solved by lattice reduction or the Bleichenbacher attack.
\section{The Bleichenbacher Approach to the HNP}
\begin{definition}
    Let $X$ be a random variable over $\Zbb \ q \Zbb$. Define \textit{bias} of $X$ as 
    \begin{equation}
        B(X) = E(e^{2\pi i X / q}) = B(X \mod q).
        \tag{5}
        \end{equation}
        For a set of points $V = (v_0, v_1, \dots, v_{L-1})$ in $\mathbb{Z}/q\mathbb{Z}$, define the \textit{sampled bias} as
        \begin{equation}
        B(V) = \frac{1}{L} \sum_{j=0}^{L-1} e^{2\pi i v_j / q}.
        \tag{6}
    \end{equation}

    \begin{lemma}
        Let $X$ be uniformly distributed on $[-(T-1)/2, \ldots, (T-1)/2]$ for some bound $0 < T \leq q$, then
        \begin{enumerate}[label=\arabic*.]
            \item For independent random variables \(X\) and \(Y\), \(B(X + Y) = B(X)B(Y)\).
            \item \(B(X) = \frac{1}{T} \sin\left(\frac{\pi T / q}{\sin(\pi / q)}\right)\). So \(B(X)\) is real-valued and \(0 \leq B(X) \leq 1\).
            \item If $T = q$, then \(B(X) = 0\).
            \item Let \(a\) be an integer with \(|a|T \leq q\), and \(Y = aX\). Then \(B(Y) = \frac{1}{T} \sin\left(\frac{\pi a T / q}{\sin(\pi a / q)}\right)\).
            \item \(B(Y) \leq B(X)^{|a|}\).
        \end{enumerate} 
    \end{lemma}
\end{definition}

\begin{example}[Bias estimation]
    
\end{example}
The idea of the Bleichenbacher attack is the following. Take a guess for the secret key $\omega \in \mathbb{Z}_q$ and let $B(\omega)$ be the bias of the set $\{h_j + c_j \cdot \omega \mod q\}$. Then we expect $\omega = x$ to be the unique number such that the bias $B(\omega)$ will be significantly nonzero, while for all other $\omega \not=x$ the bias should be close to zero. To see this compute 
\begin{align}
    B_q(\omega) 
    &= \frac{1}{L} \sum_{j=0}^{L-1} e^{2\pi i (h_j + c_j \omega)/q} 
    = \sum_{t=0}^{q-1} \left( \frac{1}{L} \sum_{\substack{\{j \mid c_j = t\}}} e^{2\pi i h_j / q} \right) e^{2\pi i t \omega / q} \notag\\
    &= \sum_{t=0}^{q-1} \left( \frac{1}{L} \sum_{\substack{\{j \mid c_j = t\}}} e^{2\pi i (h_j + c_j x)/q} \right) e^{2\pi i t (\omega - x)/q} \notag\\
    &= \sum_{t=0}^{q-1} \left( \frac{1}{L} \sum_{\substack{\{j \mid c_j = t\}}} e^{2\pi i k_j / q} \right) e^{2\pi i t (\omega - x)/q}. \label{eqBiasVypocet}
\end{align} 

When $\omega = x$, $B(\omega) = \frac{1}{L} \sum_{j=0}^{L-1} e^{2\pi i k_j / q}$ is exactly the sampled bias of the $k_j$s. Assuming a $b$-bit leak and $L$ large enough, $B(X)$ will be close to $1$, since the points $e^{2\pi i k_j /q}$ lie in the part of the unit circle with phase $-\pi/2^b < \theta < \pi/2^b$. $B(\omega)$ will be close to zero for $\omega \not =x$, since the points will be distributed over the whole circle because of the $e^{2\pi i t (\omega - x)/q}$ term in \eqref{eqBiasVypocet}.

Now evaluating this sum for all $\omega \in \Zbb_q$ is not feasible. Notice from \eqref{eqBiasVypocet} that $B(\omega)$ is a sum of terms $e^{2\pi i t \omega/q}$ with frequencies $t/q$. If the frequencies $t/q$ are much smaller than $1$, then the peak of $B(\omega)$ will broaden allowing us to search only over a sparse set of $\omega$. To achieve this we need to reduce the size of the $c_j$s. Assuming that $c_j < C$ for some $C$, and letting $n = 2C$, we can find the $n$ most significant bits of $x$ by searching for a peak in $n$ evenly spaced values of $\omega \in \mathbb{Z}_q$. Set $\omega_m = mq/n, m\in [0,n-1]$. Then 

\begin{align}
    B_q(\omega_m) 
    &= \frac{1}{L} \sum_{j=0}^{L-1} e^{2\pi i (h_j + (c_j m q / n))/q} 
    = \frac{1}{L} \sum_{j=0}^{L-1} e^{(2\pi i h_j / q) + (2\pi i c_j m / n)} \notag \\
    &= \sum_{t=0}^{n-1} \left( \frac{1}{L} \sum_{\substack{\{j \mid c_j = t\}}} e^{2\pi i h_j / q} \right) e^{2\pi i t m / n} 
    = \sum_{t=0}^{n-1} Z_t e^{2\pi i t m / n}. \tag{8}
\end{align}
is the inverse FFT of $Z = (Z_0, \ldots, Z_{n-1})$. Find the $m$ for which $B(\omega_m)$ is maximal, then the most significant $n$ bits of $x$ are $msb_n(x) = msb_n(mq/n)$. So $n$ is determined by the maximum FFT we can compute. If we can reduce the $c_j$ below $C$, then we can iteratively recover the whole secret key.

\subsection{Range reduction}
There are various approaches to range reduction. The original Bleichenbacher presentation proposes the sort and difference algorithm.

\begin{enumerate}
    \item Sort the list $\{(h_i, s_i)\}_{i=0}^{L-1}$ in ascending order by the $h_i$ values.
    \item Take the successive differences to create a new list $\{(h_i', s_i')\}_{i=0}^{L-2} = \{(h_{i+1} - h_i, s_{i+1} - s_i)\}_{i=0}^{L-2}$.
    \item Repeat.
\end{enumerate}

In the original presentation, Bleichenbacher mentioned the use of the Schroeppel–Shamir algorithm, a knapsack problem solver, for the range reduction phase. The paper (newbleichenbahcer records) transforms the Schroeppel–Shamir knapsack solver into a range reduction algorithm. The idea is to 

\begin{enumerate}
    \item Split the set $S$ of $2^{\alpha +2}$ of input signatures into $4$ lists $\mathcal{L}^1, \mathcal{R}^1, \mathcal{L}^2, \mathcal{R}^2$ of size $2^\alpha$.
    \item Fix a $c \in [0,2^\alpha] \cap  \mathbb{Z}$ and create lists $\mathcal{A}^r, r \in 1,2$, that contain the combinations of two samples $(\eta^r, \xi^r) = \mathcal{L}^r(i) + \mathcal{R}^r(j) = (c_i^r + c_j^r, h_i^r + h_j^r)$ such that $\eta^r$'s $(\alpha + 1)$ most significant bits are the same as $c$'s, that is $MSB_{\alpha + 1}(\eta^r) = MSB_{\alpha + 1}(c)$. 
    \item Sort $\mathcal{A}^1, \mathcal{A}^2$ by the first coordinate and extract short combinations.
\end{enumerate}
pseudokod je v (new bleichanbcher)









\section{Solving the HNP with Lattices}
Boneh and Vankatesan construct the following lattice for solving the HNP
\begin{equation}
    \begin{bmatrix}
    n & 0 & 0 & \cdots & 0 & 0 \\
    0 & n & 0 & \cdots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\
    0 & 0 & 0 & \cdots & n & 0 \\
    t_0 & t_1 & t_2 & \cdots & t_{m-1} & \frac{1}{n}
    \end{bmatrix},
\end{equation}
and we want to find the vector $(a_0, \ldots, a_{m-1}, 0)$. The vector $([t_0 \cdot \alpha]_p, \dots, [t_{m-1}\alpha]_p, \alpha/n)$ is within $\sqrt{m+1}\cdot 2^l$ of the desired vector for $|k_i| < 2^l$.
